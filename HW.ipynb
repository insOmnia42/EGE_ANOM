{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72bc483c",
   "metadata": {},
   "source": [
    "# Кластеризация ЕГЭ (4 если сделаны все задачи)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bc2fba",
   "metadata": {},
   "source": [
    "Рядом лежат данные с координатами точек. Везде используется Евклидово расстояние. Кластером считается набор не менее чем из 30 точек связанных друг с другом. Аномалия это точка находящаяся на расстоянии более 1 от любого кластера.\n",
    "\n",
    "* Постройте Распределение точек\n",
    "* Напишите руками DBSCAN и обработайте им все файлы\n",
    "* Файл 0.xls также решите руками\n",
    "* Постройте Распределение точек, отметьте принадлежность кластеров цветами\n",
    "* Отметьте Аномалии отдельным цветом\n",
    "* Найдите среди в каждом кластере точку расстояние от которой до всех остальных минимально\n",
    "* Выведите два числа - Среднее абсцисс и ординат центроидов кластеров * 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cb6a6519",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3df8814",
   "metadata": {},
   "source": [
    "# Кластеризация (1 за каждый алгоритм на всех данных)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf53319",
   "metadata": {},
   "source": [
    "На предложенных распределениях данных проверьте предложенные алгоритмы. Постройте графики кластеризации для каждой пары алгоритм-данные, разные кластеры покрасьте разным цветом. Воспользуйтесь sklearn реализациями. Параметры кластеризации для разных алгоритмов подберите такие, чтобы алгоритмы можно было сравнивать (по возможности одинаковое количество кластеров и т.д.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e323b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import cluster, datasets, mixture\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from itertools import cycle, islice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f18470f",
   "metadata": {},
   "outputs": [],
   "source": [
    "two_means =  \n",
    "dbscan = \n",
    "\n",
    "clustering_algorithms = (\n",
    "        (\"MeanShift\", ms),\n",
    "        (\"Spectral\\nClustering\", spectral),\n",
    "        (\"Ward\", ward),\n",
    "        (\"Agglomerative\\nClustering\", average_linkage),\n",
    "        (\"OPTICS\", optics),\n",
    "        (\"Gaussian\\nMixture\", gmm),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae73f346",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "# ============\n",
    "# Generate datasets. We choose the size big enough to see the scalability\n",
    "# of the algorithms, but not too big to avoid too long running times\n",
    "# ============\n",
    "n_samples = 1500\n",
    "noisy_circles = datasets.make_circles(n_samples=n_samples, factor=0.5, noise=0.05)\n",
    "noisy_moons = datasets.make_moons(n_samples=n_samples, noise=0.05)\n",
    "blobs = datasets.make_blobs(n_samples=n_samples, random_state=8)\n",
    "no_structure = np.random.rand(n_samples, 2), None\n",
    "\n",
    "# Anisotropicly distributed data\n",
    "random_state = 170\n",
    "X, y = datasets.make_blobs(n_samples=n_samples, random_state=random_state)\n",
    "transformation = [[0.6, -0.6], [-0.4, 0.8]]\n",
    "X_aniso = np.dot(X, transformation)\n",
    "aniso = (X_aniso, y)\n",
    "\n",
    "# blobs with varied variances\n",
    "varied = datasets.make_blobs(\n",
    "    n_samples=n_samples, cluster_std=[1.0, 2.5, 0.5], random_state=random_state\n",
    ")\n",
    "\n",
    "# ============\n",
    "# Set up cluster parameters\n",
    "# ============\n",
    "plt.figure(figsize=(9 * 2 + 3, 13))\n",
    "plt.subplots_adjust(\n",
    "    left=0.02, right=0.98, bottom=0.001, top=0.95, wspace=0.05, hspace=0.01\n",
    ")\n",
    "\n",
    "plot_num = 1\n",
    "\n",
    "default_base = {\n",
    "    \"quantile\": 0.3,\n",
    "    \"eps\": 0.3,\n",
    "    \"damping\": 0.9,\n",
    "    \"preference\": -200,\n",
    "    \"n_neighbors\": 10,\n",
    "    \"n_clusters\": 3,\n",
    "    \"min_samples\": 20,\n",
    "    \"xi\": 0.05,\n",
    "    \"min_cluster_size\": 0.1,\n",
    "}\n",
    "\n",
    "datasets = [\n",
    "    (\n",
    "        noisy_circles,\n",
    "        {\n",
    "            \"damping\": 0.77,\n",
    "            \"preference\": -240,\n",
    "            \"quantile\": 0.2,\n",
    "            \"n_clusters\": 2,\n",
    "            \"min_samples\": 20,\n",
    "            \"xi\": 0.25,\n",
    "        },\n",
    "    ),\n",
    "    (noisy_moons, {\"damping\": 0.75, \"preference\": -220, \"n_clusters\": 2}),\n",
    "    (\n",
    "        varied,\n",
    "        {\n",
    "            \"eps\": 0.18,\n",
    "            \"n_neighbors\": 2,\n",
    "            \"min_samples\": 5,\n",
    "            \"xi\": 0.035,\n",
    "            \"min_cluster_size\": 0.2,\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        aniso,\n",
    "        {\n",
    "            \"eps\": 0.15,\n",
    "            \"n_neighbors\": 2,\n",
    "            \"min_samples\": 20,\n",
    "            \"xi\": 0.1,\n",
    "            \"min_cluster_size\": 0.2,\n",
    "        },\n",
    "    ),\n",
    "    (blobs, {}),\n",
    "    (no_structure, {}),\n",
    "]\n",
    "\n",
    "for i_dataset, (dataset, algo_params) in enumerate(datasets):\n",
    "    # update parameters with dataset-specific values\n",
    "    params = default_base.copy()\n",
    "    params.update(algo_params)\n",
    "\n",
    "    X, y = dataset\n",
    "\n",
    "    # normalize dataset for easier parameter selection\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "\n",
    "    # estimate bandwidth for mean shift\n",
    "    bandwidth = cluster.estimate_bandwidth(X, quantile=params[\"quantile\"])\n",
    "\n",
    "    # connectivity matrix for structured Ward\n",
    "    connectivity = kneighbors_graph(\n",
    "        X, n_neighbors=params[\"n_neighbors\"], include_self=False\n",
    "    )\n",
    "    # make connectivity symmetric\n",
    "    connectivity = 0.5 * (connectivity + connectivity.T)\n",
    "\n",
    "    for name, algorithm in clustering_algorithms:\n",
    "        t0 = time.time()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
